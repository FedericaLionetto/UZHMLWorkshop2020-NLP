{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Machine Learning journey from customer reviews to business insights\n",
    "# *Part 2: Data preparation for review text*\n",
    "\n",
    "*Author: Federica Lionetto*  \n",
    "*Email: federica.lionetto@gmail.com*  \n",
    "*Date: 17 November 2020*  \n",
    "*License: Creative Commons BY-NC-SA*\n",
    "\n",
    "*Based on the dataset available at:*\n",
    "- https://www.kaggle.com/efehandanisman/skytrax-airline-reviews\n",
    "\n",
    "### Further readings\n",
    "\n",
    "- Hutto, C.J. and Gilbert, E.E., 2014, \"VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text\", Eighth International Conference on Weblogs and Social Media (ICWSM-14), Ann Arbor, MI, June 2014, https://www.researchgate.net/publication/275828927_VADER_A_Parsimonious_Rule-based_Model_for_Sentiment_Analysis_of_Social_Media_Text\n",
    "- Sentiment analysis using VADER, https://github.com/cjhutto/vaderSentiment\n",
    "- \"Detecting bad customer reviews with NLP\", https://towardsdatascience.com/detecting-bad-customer-reviews-with-nlp-d8b36134dc7e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Import modules and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_palette('Set2')\n",
    "\n",
    "import datetime as dt\n",
    "import dateutil\n",
    "\n",
    "import string\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import nltk\n",
    "from nltk import tokenize, pos_tag\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import os\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging capabilities.\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed for Colab.\n",
    "!git clone https://github.com/FedericaLionetto/UZHMLWorkshop2020-NLP\n",
    "os.chdir('UZHMLWorkshop2020-NLP/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys  \n",
    "sys.path.insert(0, './helper_functions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Related to visualization.\n",
    "import plot_cmap\n",
    "import plot_two_hists_comp_sns\n",
    "\n",
    "# Related to NLP.\n",
    "import get_wordnet_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Load the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type of each field in the input data.\n",
    "df_dtype = pd.read_csv('../Results/PreprocessedDataLightTypes.csv')\n",
    "dict_dtype = df_dtype[['index','dtypes']].set_index('index').to_dict()['dtypes']\n",
    "dict_dtype['recommended'] = 'bool'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data.\n",
    "df = pd.read_csv('../Results/PreprocessedDataLight.csv', dtype=dict_dtype, keep_default_na=False, na_values=['_'])\n",
    "df.drop(columns=['Unnamed: 0'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the names of the colums in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cols = df.columns.to_list()\n",
    "print('Columns in the dataset:')\n",
    "print(cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the total number of customer reviews in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_reviews = df.shape[0]\n",
    "print('Number of customer reviews in the dataset: {:d}'.format(n_reviews))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Work with the review text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 - Get review text and create a new data frame with NLP information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Series of all review texts in the dataset.\n",
    "reviews_list = df['review_text'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nlp = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - Sentiment analysis using VADER\n",
    "\n",
    "Sentiment analysis is the field of NLP that aims at understanding the sentiment of a certain portion of text. One of the best-known packages for sentiment analysis is the open-source package VADER, which is part of NLTK.\n",
    "\n",
    "The official description of VADER reads\n",
    "\"VADER (Valence Aware Dictionary and sEntiment Reasoner) is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media.\"   \n",
    "VADER is built on social media text but it is in general applicable to other domains, including customer reviews.  \n",
    "VADER is based on a lexicon (vocabulary) that is validated by multiple human judges according to a well-defined and standard procedure. Each word in the lexicon is associated with a sentiment valence, consisting of two properties, polarity and intensity. The polarity describes if the text is positive/negative. The intensity describes how much the text is positive/negative, on a scale from -4 to 4. Words not included in the lexicon are classified as neutral. \n",
    "\n",
    "To evaluate the sentiment of a sentence or list of sentences, VADER looks for words in the text that are part of the lexicon, modifies the intensity and polarity of the identified words according to a series of rules, sums up these values and then normalises to the range [-1,1].  \n",
    "VADER incorporates emojis (for example \":-)\"), acronyms (for example \"LOL\") and slang (for example \"nah\"). The algorithm differs from a Bag of Words approach as it takes words order and degree modifiers into account, e.g. by increasing/decreasing the intensity of the sentiment.   \n",
    "For example, the sentences:\n",
    "- \"This flight was great.\", \n",
    "- \"This flight was really great.\" \n",
    "- \"This flight was really GREAT.\"\n",
    "- \"This flight was really GREAT!\"\n",
    "- \"This flight was really GREAT! :-)\"  \n",
    "would have an increasing intensity, triggered by degree modifiers.\n",
    "\n",
    "The output of the sentiment analysis is a series of scores, namely \"compound\", \"pos\", \"neu\" and \"neg\".  \n",
    "The compound score is normalized between -1 (extremely negative) and 1 (extremely positive) and is a good metric if we need a single value that summarises the sentiment of a given sentence. The compound score can also be used to classify sentences into positive, neutral and negative by setting an appropriate threshold on the compound score. The official recommended threshold is:\n",
    "- positive sentiment, compound score >= 0.05\n",
    "- neutral sentiment, compound score <= 0.05 and >= -0.05\n",
    "- negative sentiment, compound score <= -0.05  \n",
    "\n",
    "The positive, neutral and negative scores represent the fraction of the sentence that has a positive, neutral and negative sentiment. The sum of these three scores should sum up to 1. The positive, neutral and negative scores are a good metric if we need multiple values that summarise the sentiment of a given sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple examples.\n",
    "print(sid.polarity_scores(\"This flight was great.\"))\n",
    "print(sid.polarity_scores(\"This flight was really great.\"))\n",
    "print(sid.polarity_scores(\"This flight was really GREAT.\"))\n",
    "print(sid.polarity_scores(\"This flight was really GREAT!\"))\n",
    "print(sid.polarity_scores(\"This flight was really GREAT! :-)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples.\n",
    "review = reviews_list[0]\n",
    "review_tok = tokenize.sent_tokenize(review)\n",
    "print(review_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example on a review level.\n",
    "print('Review text:')\n",
    "print(review)\n",
    "\n",
    "review_polarity_scores = sid.polarity_scores(review)\n",
    "\n",
    "for key in sorted(review_polarity_scores.keys()):\n",
    "    print('{}: {}, '.format(key,review_polarity_scores[key]), end='')\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example on a sentence level.\n",
    "print('Review text:')\n",
    "print(review_tok)\n",
    "\n",
    "for sentence in review_tok:\n",
    "    print('Sentence text:')\n",
    "    print(sentence)\n",
    "    sentence_polarity_scores = sid.polarity_scores(sentence)\n",
    "\n",
    "    for key in sorted(sentence_polarity_scores.keys()):\n",
    "        print('{}: {}, '.format(key,sentence_polarity_scores[key]), end='')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augment the dataset with the overall polarity score of the review, as obtained using VADER on the review level.\n",
    "reviews_polarity = []\n",
    "\n",
    "for i_review, review in enumerate(reviews_list):\n",
    "    # print('Review text:')\n",
    "    # print(review)\n",
    "\n",
    "    review_polarity_scores = sid.polarity_scores(review)\n",
    "    review_polarity_score_compound = review_polarity_scores['compound']\n",
    "    \n",
    "    print('Review #{:d}: '.format(i_review), end='')\n",
    "    for key in sorted(review_polarity_scores.keys()):\n",
    "        print('{}: {:.4f}, '.format(key,review_polarity_scores[key]), end='')\n",
    "    print('')\n",
    "    \n",
    "    reviews_polarity.append(review_polarity_score_compound)\n",
    "\n",
    "# print(reviews_polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nlp['polarity'] = reviews_polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_nlp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We look at the correlation between compound score and recommendation and at the distribution of the compound score for positive and negative customer reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_values = df_nlp[['polarity','recommended']].dropna(axis=0,how='any').corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cmap.plot_cmap(matrix_values=corr_values, \n",
    "                    figsize_w=4, \n",
    "                    figsize_h=4, \n",
    "                    filename='../Results/02/Corr.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_two_hists_comp_sns.plot_two_hists_comp_sns(df_1=df_nlp[df_nlp['recommended']==True],\n",
    "                                                df_2=df_nlp[df_nlp['recommended']==False],\n",
    "                                                label_1='recommended',\n",
    "                                                label_2='not recommended',\n",
    "                                                feat='polarity',\n",
    "                                                bins=30,\n",
    "                                                title='Distribution of all customer reviews',\n",
    "                                                x_label='Polarity',\n",
    "                                                y_label='Entries / bin',\n",
    "                                                filename='../Results/02/HistPolarityByRecommendation.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DISCUSSION**:  \n",
    "*What could be the limitations of this approach? Would you expect it to perform well on the customer reviews?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 - Preprocess review text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.1 - Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.2 - Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop words.\n",
    "# Airlines appearing in the dataset. This is the official name of the airlines. These words should be removed from the review text.\n",
    "airlines_lower = df_nlp['airline'].str.lower().unique().tolist()\n",
    "# Words appearing in the official name of the airlines. These words should be removed from the review text.\n",
    "airlines_identifier = ['airlines',\n",
    "                       'air lines',\n",
    "                       'airline',\n",
    "                       'air line',\n",
    "                       'airways',\n",
    "                       'air']\n",
    "# In addition to the official name of the airlines, customers can use shortened versions of this name.\n",
    "airlines_informal_lower = []\n",
    "for airline in airlines_lower:\n",
    "    found = False\n",
    "    for airline_identifier in airlines_identifier:\n",
    "        if found == False:\n",
    "            if str(' '+airline_identifier) in airline:\n",
    "                airline_informal = airline.replace(str(' '+airline_identifier),'')\n",
    "                airlines_informal_lower.append(airline_informal)\n",
    "                found = True\n",
    "# Other stop words.\n",
    "additional_stopwords = ['one','get','also','however','even','make']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(airlines_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(airlines_identifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(airlines_informal_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nltk_stopwords = stopwords.words('english')\n",
    "nltk_stopwords_extended = nltk_stopwords + airlines_lower + airlines_identifier + airlines_informal_lower + additional_stopwords\n",
    "print('Number of stopwords in NLTK: {:d}'.format(len(nltk_stopwords)))\n",
    "print('Number of stopwords after extension: {:d}'.format(len(nltk_stopwords_extended)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.3 - Lower/upper case, punctuation, tokenization, stop words, POS tagging and lemmatization\n",
    "\n",
    "First of all, we convert all characters in the review text to lower case.\n",
    "\n",
    "After that, we remove the punctuation and tokenize each customer review into a list of individual words. \n",
    "\n",
    "As a next step, we need to select only those words in the review text that could be relevant to solve the problem at hand. In particular, all stop words should be filtered out as they do not affect the meaning of the sentence.  \n",
    "We can download the stopwords from NLTK and specify that we want to use those corresponding to the English language.\n",
    "\n",
    "We then proceed to POS tagging, which allows to identify the role of each word in the sentence, according to the categories noun, verb, adjective, adverb and others. This is needed for a correct lemmatization of the words in the review text.\n",
    "\n",
    "The lemmatization consists in bringing the words to their \"standard\" form, e.g. to convert \"wrote\" to \"write\" or \"writing\" to write."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean_text(text):\n",
    "    # Transform the text so that all words are lower case.\n",
    "    # print(text)\n",
    "    text = text.lower()\n",
    "    # Remove stop words corresponding to airlines. This is needed here as airline names can consist of multiple words and will not be removed after splitting by words.\n",
    "    # print(text)\n",
    "    for airline_lower in airlines_lower:\n",
    "        text = text.replace(airline_lower, '')\n",
    "    # Remove punctuation and tokenize the text into individual words.\n",
    "    # print(text)\n",
    "    text = [word.strip(string.punctuation) for word in text.split(\" \")]\n",
    "    # Remove words that contain numbers.\n",
    "    # print(text)\n",
    "    text = [word for word in text if not any(c.isdigit() for c in word)]\n",
    "    # Remove stop words.\n",
    "    # print(text)\n",
    "    text = [word for word in text if word not in nltk_stopwords_extended]\n",
    "    # Remove empty tokens.\n",
    "    # print(text)\n",
    "    text = [word for word in text if len(word)>0]\n",
    "    # POS tagging of the text.\n",
    "    # print(text)\n",
    "    pos_tags = pos_tag(text)\n",
    "    # Lemmatize the text.\n",
    "    # print(text)\n",
    "    text = [WordNetLemmatizer().lemmatize(i_pos_tag[0], get_wordnet_pos.get_wordnet_pos(i_pos_tag[1])) for i_pos_tag in pos_tags]\n",
    "    # Remove words with only one letter.\n",
    "    # print(text)\n",
    "    text = [word for word in text if len(word)>1]\n",
    "    # Join the text with space as a word delimiter.\n",
    "    # print(text)\n",
    "    text = \" \".join(text)\n",
    "    # Remove non-ASCII characters.\n",
    "    printable = set(string.printable)\n",
    "    text = ''.join(filter(lambda x: x in printable, text))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of POS tagging.\n",
    "pos_tag(tokenize.word_tokenize('This is a simple test for you.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of lemmatization.\n",
    "WordNetLemmatizer().lemmatize('written',wordnet.VERB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_clean_text(reviews_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nlp['review_text_clean'] = df_nlp['review_text'].apply(lambda x: get_clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nlp['review_text_clean'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.4 - Vectorization\n",
    "\n",
    "We convert the text of each customer review from a textual representation to a numerical representation. The vectors of the numerical representation correspond to the words that appear in the preprocessed text of the customer reviews. The values in the numerical representation correspond to the occurrences of the specified word in the customer review. To avoid to end up with too many features in the numerical representation, we limit the dictionary to the words that appear at least a minimum number of times in the customer reviews. This threshold is specified through the parameter `min_df` of `CountVectorizer`.  \n",
    "\n",
    "For example, if we want to use a 3D numerical representation, we might have features corresponding to the words `flight`, `service`, `food`. For a certain customer review, the value of the feature `flight` will correspond to how many times `flight` is mentioned in the text, and similar for the other two features `service` and `food`.\n",
    "\n",
    "It should be noted that, up to this point, there is no \"meaning\" associated to the words in the dictionary. The numerical representation does not take the similarity between two words into account. Embeddings are a way to map words to meanings and are an interesting option, but for the sake of simplicity they are not implemented in this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of reviews.\n",
    "corpus = df_nlp['review_text_clean'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_ngrams = CountVectorizer(binary=False, ngram_range=(1, 1), analyzer='word', min_df=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vectorizer_ngrams.fit(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_review_text_clean_feats = vectorizer_ngrams.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vec_review_text_clean_feats[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vec_review_text_clean_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_review_text_clean = vectorizer_ngrams.transform(df_nlp['review_text_clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_review_text_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_review_text_clean.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_review_text_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_review_text_clean_feats_new = ['count_'+feat for feat in vec_review_text_clean_feats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add features to the dataset.\n",
    "df_vec_review_text_clean = pd.DataFrame(vec_review_text_clean.toarray(),columns=vec_review_text_clean_feats_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vec_review_text_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nlp['review_text_clean'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vec_review_text_clean.iloc[0]['count_lose']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nlp_final = pd.concat([df_nlp,df_vec_review_text_clean], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nlp_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nlp_final['cabin'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Save the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nlp_final_types = df_nlp_final.dtypes.to_frame('dtypes').reset_index()\n",
    "\n",
    "df_nlp_types = df_nlp.dtypes.to_frame('dtypes').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nlp_final.to_csv('../Results/NLPFinalDataLight.csv')\n",
    "df_nlp_final_types.to_csv('../Results/NLPFinalDataLightTypes.csv')\n",
    "\n",
    "df_nlp.to_csv('../Results/NLPDataLight.csv')\n",
    "df_nlp_types.to_csv('../Results/NLPDataLightTypes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Results/VecReviewTextCleanFeats.csv', 'w') as f:\n",
    "    f.write(', '.join(vec_review_text_clean_feats_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Results/NLTKStopWordsExtended.csv', 'w') as f:\n",
    "    f.write(', '.join(nltk_stopwords_extended))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Machine Learning journey from customer reviews to business insights\n",
    "# *Part 3: Modelling*\n",
    "\n",
    "*Author: Federica Lionetto*  \n",
    "*Email: federica.lionetto@gmail.com*  \n",
    "*Date: 17 November 2020*  \n",
    "*License: Creative Commons BY-NC-SA*\n",
    "\n",
    "*Based on the dataset available at:*\n",
    "- https://www.kaggle.com/efehandanisman/skytrax-airline-reviews\n",
    "\n",
    "### Further readings\n",
    "\n",
    "- \"What can we learn from five-star airlines: a web scraping project from Skytrax\", https://nycdatascience.com/blog/student-works/web-scraping/what-can-we-learn-from-five-star-airlines-a-web-scraping-project-from-skytrax/\n",
    "- Pipelines for data processing: https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html\n",
    "- LightGBM, https://lightgbm.readthedocs.io/en/latest/index.html\n",
    "- LightGBM parameters, https://lightgbm.readthedocs.io/en/latest/Parameters.html\n",
    "- SHAP decision plots, https://slundberg.github.io/shap/notebooks/plots/decision_plot.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 - Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_review_text = True\n",
    "use_count_vectorization = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not use_review_text:\n",
    "    # Without review text.\n",
    "    df_types_filename = '../Results/PreprocessedDataLightTypes.csv'\n",
    "    df_filename = '../Results/PreprocessedDataLight.csv'\n",
    "    df_out_filename = '../Results/Preds-WithoutText.csv'\n",
    "else:\n",
    "    # With review text.\n",
    "    df_types_filename = '../Results/NLPFinalDataLightTypes.csv'\n",
    "    df_filename = '../Results/NLPFinalDataLight.csv'\n",
    "    df_out_filename = '../Results/Preds-WithText.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define numerical and categorical features.\n",
    "if not use_review_text:\n",
    "    # Without review text.\n",
    "    num_feats = ['date_flown_month',\n",
    "                 'date_flown_year',\n",
    "                 'review_date_date_flown_distance_days',\n",
    "                 'review_characters',\n",
    "                 'has_layover_num',\n",
    "                 'seat_comfort',\n",
    "                 'cabin_service',\n",
    "                 'food_bev',\n",
    "                 'entertainment',\n",
    "                 'ground_service',\n",
    "                 'value_for_money']\n",
    "    cat_feats = ['airline',\n",
    "                 'traveller_type',\n",
    "                 'cabin']\n",
    "else:\n",
    "    # With review text.\n",
    "    if not use_count_vectorization:\n",
    "        num_feats = ['date_flown_month',\n",
    "                     'date_flown_year',\n",
    "                     'review_date_date_flown_distance_days',\n",
    "                     'review_characters',\n",
    "                     'has_layover_num',\n",
    "                     'seat_comfort',\n",
    "                     'cabin_service',\n",
    "                     'food_bev',\n",
    "                     'entertainment',\n",
    "                     'ground_service',\n",
    "                     'value_for_money',\n",
    "                     'polarity']\n",
    "    else:\n",
    "        with open('../Results/VecReviewTextCleanFeats.csv','r') as f:\n",
    "            vec_feats = f.read()\n",
    "            vec_feats = vec_feats.split(', ')\n",
    "        num_feats = ['date_flown_month',\n",
    "                     'date_flown_year',\n",
    "                     'review_date_date_flown_distance_days',\n",
    "                     'review_characters',\n",
    "                     'has_layover_num',\n",
    "                     'seat_comfort',\n",
    "                     'cabin_service',\n",
    "                     'food_bev',\n",
    "                     'entertainment',\n",
    "                     'ground_service',\n",
    "                     'value_for_money',\n",
    "                     'polarity'] + vec_feats\n",
    "    cat_feats = ['airline',\n",
    "                 'traveller_type',\n",
    "                 'cabin']\n",
    "\n",
    "feats = num_feats + cat_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this variable to the desired method for data transformation.\n",
    "# Possible options are: scaling_and_one_hot_encoding, label_encoding, no_transformation.\n",
    "transform_dataset = 'label_encoding'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Import modules and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_palette('Set2')\n",
    "import scipy.sparse\n",
    "\n",
    "import datetime as dt\n",
    "import dateutil\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.metrics import roc_curve, accuracy_score, roc_auc_score, confusion_matrix \n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "import shap\n",
    "\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging capabilities.\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed for Colab.\n",
    "!git clone https://github.com/FedericaLionetto/UZHMLWorkshop2020-NLP\n",
    "os.chdir('UZHMLWorkshop2020-NLP/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys  \n",
    "sys.path.insert(0, './helper_functions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Related to recommendation.\n",
    "import assign_label_recommended\n",
    "\n",
    "# Related to modelling.\n",
    "import plot_roc_curve\n",
    "import plot_feature_importance\n",
    "import plot_confusion_matrix\n",
    "\n",
    "# Related to visualization.\n",
    "import plot_hist_sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Load the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type of each field in the input data.\n",
    "df_dtype = pd.read_csv(df_types_filename)\n",
    "dict_dtype = df_dtype[['index','dtypes']].set_index('index').to_dict()['dtypes']\n",
    "dict_dtype['recommended'] = 'bool'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data.\n",
    "df = pd.read_csv(df_filename, dtype=dict_dtype, keep_default_na=False, na_values=['_'])\n",
    "df.drop(columns=['Unnamed: 0'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the names of the colums in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = df.columns.to_list()\n",
    "print('Columns in the dataset:')\n",
    "print(cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the total number of customer reviews in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_reviews = df.shape[0]\n",
    "print('Number of customer reviews in the dataset: {:d}'.format(n_reviews))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Predict whether the customer would recommend the product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 - Add the label to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = df.apply(lambda x: assign_label_recommended.assign_label_recommended(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - Convert Boolean features to numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['has_layover_num'] = df['has_layover'].astype(int)\n",
    "df['date_flown_day'] = df['date_flown_day'].astype(int)\n",
    "df['date_flown_month'] = df['date_flown_month'].astype(int)\n",
    "df['date_flown_year'] = df['date_flown_year'].astype(int)\n",
    "\n",
    "df['seat_comfort'] = df['seat_comfort'].astype(int)\n",
    "df['cabin_service'] = df['cabin_service'].astype(int)\n",
    "df['ground_service'] = df['ground_service'].astype(int)\n",
    "df['food_bev'] = df['food_bev'].astype(int)\n",
    "df['value_for_money'] = df['value_for_money'].astype(int)\n",
    "df['entertainment'] = df['entertainment'].astype(int)\n",
    "\n",
    "for feat in num_feats:\n",
    "    if 'polarity' not in feat:\n",
    "        df[feat] = df[feat].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 - Select features for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[feats]\n",
    "y = df['label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 - Check class balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_rec = (y[y==1].shape[0])/y.shape[0]\n",
    "f_not_rec = (y[y==0].shape[0])/y.shape[0]\n",
    "print('Fraction of customers that recommeded the service: {:.2f}'.format(f_rec))\n",
    "print('Fraction of customers that did not recommed the service: {:.2f}'.format(f_not_rec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 - Scale numerical features and apply one-hot encoding to categorical features\n",
    "\n",
    "Before feeding the selected features to the Machine Learning model, we can transform them to allow the model to correctly interpret them.  \n",
    "\n",
    "We can specify how to impute missing values. In this exercise, we use the `SimpleImputer` from `sklearn`.\n",
    "\n",
    "We might want to scale numerical features, so that they have values in a common range.   \n",
    "In this exercise, we use the `StandardScaler` available in `sklearn` to normalize the features, that is, to subtract their mean and divide by their standard deviation. We transform `x` to `z = (x-u)/s`. We can specify whether or no we want to subtract the mean with the option `with_mean=True/False` and whether or no we want to divide by the standard deviation with the option `with_std=True/False`. As a result, all the numerical features will have mean zero and unit standard deviation. \n",
    "\n",
    "In addition to numerical features, we might want to transform categorical features as well.\n",
    "Different algorithms require categorical features to have different formats. Two common options are one-hot encoding and label encoding.  \n",
    "1) One-hot encoding allows to encode categorical features as one-hot vectors. The categorical feature is transformed into binary features, one for each category. By default, the econder derives the categories based on the unique values in each feature.  \n",
    "   Let us consider the following example. The categorical feature `cabin` can have four possible values: `Economy Class`, `Premium Economy`, `Business Class` and `First Class`. The one-hot encoding transform this feature, with four possible values, into four new features, called `cabin_Economy Class`, `cabin_Premium Economy`, `cabin_Business Class` and `cabin_First Class`, with each new feature having two possible values, `0` or `1`, depending on the value of the original feature. A record with `cabin` equal to `Economy Class` will have `cabin_Economy Class` equal to `1` and all other three new features equal to `0`.  \n",
    "   This leads to sparse data (most of the elements in the dataset will have the value `0`) if the features can have many possible values.  \n",
    "2) Label encoding allows to encode categorical features as numbers.  \n",
    "   For example, the categorical feature `cabin` can be encoded as one feature with values `0`, `1`, `2` and `3`.\n",
    "\n",
    "We use a pipeline to define the data processing, so that we can repeat the same steps for the training and test datasets. In particular, the parameters of the data processing are defined based on the training dataset and are then applied to the test dataset. This is particularly important if the Machine Learning model has to be used in a live system and has to make predictions on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline for numerical features and a pipeline for categorical features.\n",
    "num_proc = make_pipeline(SimpleImputer(missing_values=np.nan, strategy='mean'), StandardScaler())\n",
    "cat_proc = make_pipeline(SimpleImputer(strategy='constant', fill_value='missing'), OneHotEncoder(handle_unknown='ignore'))\n",
    "\n",
    "# Create a preprocessing step for all features.\n",
    "preprocessor = make_column_transformer((num_proc, num_feats),\n",
    "                                       (cat_proc, cat_feats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 - Dataset split for training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6.1 - Dataset transformation before training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the training and test datasets as specified in the preprocessor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transformed = preprocessor.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The order of the features after the data processing is the same that is specified in the pipeline, in this case starting with the numerical features and continuing with the categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_feats_one_hot = preprocessor.transformers_[1][1]['onehotencoder'].get_feature_names(cat_feats)\n",
    "print(cat_feats_one_hot)\n",
    "\n",
    "all_feats = list(num_feats)+list(cat_feats_one_hot)\n",
    "print(all_feats)\n",
    "\n",
    "dict_for_renaming_cols = {}\n",
    "for i in range(len(all_feats)):\n",
    "    dict_for_renaming_cols[i] = all_feats[i]\n",
    "print(dict_for_renaming_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if scipy.sparse.issparse(X_train_transformed):\n",
    "    X_train_transformed_2 = pd.DataFrame.sparse.from_spmatrix(X_train_transformed)\n",
    "else:\n",
    "    X_train_transformed_2 = pd.DataFrame(X_train_transformed)\n",
    "X_train_transformed_2.rename(columns=dict_for_renaming_cols,inplace=True)\n",
    "\n",
    "X_test_transformed = preprocessor.transform(X_test)\n",
    "if scipy.sparse.issparse(X_test_transformed):\n",
    "    X_test_transformed_2 = pd.DataFrame.sparse.from_spmatrix(X_test_transformed)\n",
    "else:\n",
    "    X_test_transformed_2 = pd.DataFrame(X_test_transformed)\n",
    "X_test_transformed_2.rename(columns=dict_for_renaming_cols,inplace=True)\n",
    "\n",
    "X_transformed = preprocessor.transform(X)\n",
    "if scipy.sparse.issparse(X_transformed):\n",
    "    X_transformed_2 = pd.DataFrame.sparse.from_spmatrix(X_transformed)\n",
    "else:\n",
    "    X_transformed_2 = pd.DataFrame(X_transformed)\n",
    "X_transformed_2.rename(columns=dict_for_renaming_cols,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transformed_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_transformed_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_transformed_2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6.1 - Dataset transformation before training according to label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb_make = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_label_enc = X.copy()\n",
    "X_train_label_enc = X_train.copy()\n",
    "X_test_label_enc = X_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feat in cat_feats:\n",
    "    print('Feature:', feat)\n",
    "    X_label_enc[feat] = lb_make.fit_transform(X_label_enc[feat])\n",
    "    X_train_label_enc[feat] = lb_make.fit_transform(X_train_label_enc[feat])\n",
    "    X_test_label_enc[feat] = lb_make.fit_transform(X_test_label_enc[feat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_label_enc[cat_feats].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 - Model training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if transform_dataset == 'scaling_and_one_hot_encoding':\n",
    "    print('Method for data tranformation: scaling and one hot encoding')\n",
    "    X_train_for_model = X_train_transformed_2\n",
    "    X_test_for_model = X_test_transformed_2\n",
    "    X_for_model = X_transformed_2\n",
    "    X_test_for_shap = X_test_transformed_2\n",
    "    X_for_shap = X_transformed_2\n",
    "elif transform_dataset == 'label_encoding':\n",
    "    print('Method for data transformation: label encoding')\n",
    "    X_train_for_model = X_train_label_enc\n",
    "    X_test_for_model = X_test_label_enc\n",
    "    X_for_model = X_label_enc\n",
    "    X_test_for_shap = X_test_label_enc\n",
    "    X_for_shap = X_label_enc\n",
    "elif transform_dataset == 'no_transformation':\n",
    "    print('Method for data transformation: no transformation')\n",
    "    X_train_for_model = X_train\n",
    "    X_test_for_model = X_test \n",
    "    X_for_model = X\n",
    "    X_test_for_shap = X_test\n",
    "    X_for_shap = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.7.1 - Training and test on transformed features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# LightGBM.\n",
    "if transform_dataset == 'scaling_and_one_hot_encoding':\n",
    "    train_data=lgb.Dataset(X_train_for_model,label=y_train)\n",
    "    test_data=lgb.Dataset(X_test_for_model,label=y_test)\n",
    "elif transform_dataset == 'label_encoding':    \n",
    "    train_data=lgb.Dataset(X_train_for_model,label=y_train,categorical_feature=cat_feats)\n",
    "    test_data=lgb.Dataset(X_test_for_model,label=y_test,categorical_feature=cat_feats)\n",
    "elif transform_dataset == 'no_transformation':\n",
    "    train_data=lgb.Dataset(X_train_for_model,label=y_train)\n",
    "    test_data=lgb.Dataset(X_test_for_model,label=y_test)\n",
    "else:\n",
    "    train_data=lgb.Dataset(X_train_for_model,label=y_train)\n",
    "    test_data=lgb.Dataset(X_test_for_model,label=y_test)\n",
    "    \n",
    "# Hyper-parameters.\n",
    "params = {'metric': 'binary_logloss', # Possible options are 'auc', 'binary_logloss', 'multi_logloss'.\n",
    "          'boosting_type': 'gbdt', # Gradient boosting decision tree.\n",
    "          'objective': 'binary', # 'binary' for binary classification, 'multiclass' for multi classification, 'regression' for regression.\n",
    "          'feature_fraction': 0.5,\n",
    "          'num_leaves': 30,\n",
    "          'max_depth': -1,\n",
    "          'n_estimators': 200,\n",
    "          'min_data_in_leaf': 100, \n",
    "          # 'min_child_weight': 0.1,\n",
    "          'reg_alpha': 2,\n",
    "          'reg_lambda': 5,\n",
    "          'subsample': 0.8,\n",
    "          'verbose': -1,\n",
    "          # 'num_class': 4 # Number of classes minus 1 for multiclass classification.\n",
    "          # 'num_threads': 4\n",
    "}\n",
    "\n",
    "lgbm = lgb.train(params,\n",
    "                 train_data,\n",
    "                 2500, # Epochs.\n",
    "                 valid_sets=test_data,\n",
    "                 early_stopping_rounds= 30,\n",
    "                 verbose_eval= 10\n",
    "                 )\n",
    "\n",
    "y_prob = lgbm.predict(X_for_model)\n",
    "y_pred = y_prob.round(0)\n",
    "\n",
    "clf_roc_auc_score = roc_auc_score(y, y_prob)\n",
    "clf_accuracy_score = accuracy_score(y, y_pred)\n",
    "\n",
    "print('Model overall ROC AUC score: {:.3f}'.format(clf_roc_auc_score))\n",
    "print('Model overall accuracy: {:.3f}'.format(clf_accuracy_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-checks.\n",
    "print('Min value of prediction: {:.3f}'.format(y_pred.min()))\n",
    "print('Max value of prediction: {:.3f}'.format(y_pred.max()))\n",
    "print('Min value of probability: {:.3f}'.format(y_prob.min()))\n",
    "print('Max value of probability: {:.3f}'.format(y_prob.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(y==0))\n",
    "print(np.sum(y==1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_accuracy = np.sum(y==1)/y.shape[0]\n",
    "print('Accuracy of dummy classifier: %.2f' % dummy_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y, y_pred).ravel()\n",
    "\n",
    "sensitivity = tp / (tp+fn) # Recall.\n",
    "specificity = tn / (tn+fp)\n",
    "precision = tp / (tp+fp)\n",
    "\n",
    "print('Sensitivity/Recall: %.2f' % sensitivity)\n",
    "print('Specificity: %.2f' % specificity)\n",
    "print('Precision: %.2f' % precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix.plot_confusion_matrix(y=y, y_pred=y_pred, normalize_str='true', figsize_w=4, figsize_h=4, filename='../Results/03/ConfusionMatrix.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True positive rate and false positive rate.\n",
    "fpr, tpr, _ = roc_curve(y, y_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve.plot_roc_curve(fpr=fpr, tpr=tpr, clf_name='LightGBM', figsize_w=6, figsize_h=6, filename='../Results/03/ROCCurve.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_names = lgbm.feature_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_importances = lgbm.feature_importance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_indices = np.argsort(feats_importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "# print(\"Feature ranking:\")\n",
    "\n",
    "# for f in range(50):\n",
    "#     print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features_names = []\n",
    "top_features_importances = []\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(min(30,len(feats_importances))):\n",
    "    top_features_names.append(feats_names[feats_indices[f]])\n",
    "    top_features_importances.append(feats_importances[feats_indices[f]])\n",
    "    print(\"%d. feature %d - %s (%f)\" % (f + 1, feats_indices[f], feats_names[feats_indices[f]], feats_importances[feats_indices[f]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importance.plot_feature_importance(features_names=top_features_names, features_importances=top_features_importances, figsize_w=6, figsize_h=6, filename='../Results/03/FeatureImportance.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importance.\n",
    "ax = lgb.plot_importance(lgbm, max_num_features=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lgb.create_tree_digraph(lgbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out = pd.DataFrame()\n",
    "df_out['y_pred'] = y_pred\n",
    "df_out['y_prob'] = y_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist_sns.plot_hist_sns(df=df_out,\n",
    "             feat='y_prob',\n",
    "             bins=30,\n",
    "             title='Distribution of model prediction',\n",
    "             x_label='Predicted probability of being recommended',\n",
    "             y_label='Entries / bin',\n",
    "             filename='../Results/03/HistModelPredictions.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8 - Model explainability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.8.1 - Explainer, expected value, SHAP values and SHAP interaction values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(model=lgbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the first element of the shap values array. This refers to the prediction of the model.\n",
    "shap_values_test = explainer.shap_values(X_test_for_shap)\n",
    "shap_values = explainer.shap_values(X_for_shap)\n",
    "if isinstance(shap_values_test, list):\n",
    "    shap_values_test = shap_values_test[1]\n",
    "if isinstance(shap_values, list):\n",
    "    shap_values = shap_values[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if transform_dataset=='scaling_and_one_hot_encoding':\n",
    "    shap_int_values_test = explainer.shap_interaction_values(X_test_for_shap)\n",
    "    shap_int_values = explainer.shap_interaction_values(X_for_shap)\n",
    "    if isinstance(shap_int_values_test, list):\n",
    "        shap_int_values_test = shap_int_values_test[1]\n",
    "    if isinstance(shap_int_values, list):\n",
    "        shap_int_values = shap_int_values[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shap_values[0].shape\n",
    "# shap_values[1].shape\n",
    "# shap_int_values_test.shape\n",
    "# shap_int_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_prob_for_expected_value = 0.5\n",
    "default_prob_for_expected_value_pos = 0.999\n",
    "default_prob_for_expected_value_neg = 0.001\n",
    "\n",
    "default_expected_value = np.log(default_prob_for_expected_value / (1 - default_prob_for_expected_value)) \n",
    "default_expected_value_pos = np.log(default_prob_for_expected_value_pos / (1 - default_prob_for_expected_value_pos)) \n",
    "default_expected_value_neg = np.log(default_prob_for_expected_value_neg / (1 - default_prob_for_expected_value_neg)) \n",
    "\n",
    "expected_value = explainer.expected_value\n",
    "\n",
    "if isinstance(expected_value, list):\n",
    "    expected_value = expected_value[1]\n",
    "print('Explainer expected value: {:.2f}'.format(expected_value))\n",
    "\n",
    "if expected_value is None:\n",
    "    expected_value = default_expected_value\n",
    "    \n",
    "expected_value_pos = default_expected_value_pos\n",
    "expected_value_neg = default_expected_value_neg\n",
    "    \n",
    "print('Expected value used in the plots: {:.2f} for all records, {:.2f} for strong recommended and {:.2f} for strong not recommended'.format(expected_value, expected_value_pos, expected_value_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th_strong_rec = 0.995\n",
    "th_strong_not_rec = 0.005\n",
    "th_misclassified = 0.8\n",
    "\n",
    "# Subset corresponding to high probability to recommend.\n",
    "X_strong_rec = X_for_shap[y_prob>th_strong_rec]\n",
    "shap_values_strong_rec = shap_values[y_prob>th_strong_rec]\n",
    "# Subset corresponding to low probability to recommend.\n",
    "X_strong_not_rec = X_for_shap[y_prob<th_strong_not_rec]\n",
    "shap_values_strong_not_rec = shap_values[y_prob<th_strong_not_rec]\n",
    "# Misclassified records.\n",
    "X_misclassified = X_for_shap[np.abs(y_prob-y)>th_misclassified]\n",
    "shap_values_misclassified = shap_values[np.abs(y_prob-y)>th_misclassified]\n",
    "\n",
    "n_strong_rec = X_strong_rec.shape[0]\n",
    "n_strong_not_rec = X_strong_not_rec.shape[0]\n",
    "n_misclassified = X_misclassified.shape[0]\n",
    "\n",
    "print('Number of customer reviews with prediction of recommendation > {:.3f}: {:d}'.format(th_strong_rec,n_strong_rec))\n",
    "print('Number of customer reviews with prediction of recommendation < {:.3f}: {:d}'.format(th_strong_not_rec,n_strong_not_rec))\n",
    "print('Number of customer reviews with misclassified prediction of recommendation: {:d}'.format(n_misclassified))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.8.2 - Summary plot\n",
    "\n",
    "The **summary plot** shows the feature importance based on the SHAP values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_for_shap, plot_type='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values[df['traveller_type']=='Business'], X_for_shap[df['traveller_type']=='Business'], plot_type='bar')\n",
    "shap.summary_plot(shap_values[df['traveller_type']=='Family Leisure'], X_for_shap[df['traveller_type']=='Family Leisure'], plot_type='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_for_shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_for_shap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.8.3 - Dependence plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **dependence plot** allows to visualize how the SHAP value associated to a certain feature changes as a function of the value of that feature. The color scale adds information on the value of a different feature, showing possible interactions between the two features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'value_for_money' in X_for_shap.columns.tolist():\n",
    "    if transform_dataset=='scaling_and_one_hot_encoding':\n",
    "        shap.dependence_plot('value_for_money', shap_values, X_for_shap, interaction_index='cabin_Economy Class')\n",
    "        shap.dependence_plot('value_for_money', shap_values, X_for_shap, interaction_index='cabin_Business Class')\n",
    "    elif transform_dataset=='label_encoding':\n",
    "        shap.dependence_plot('value_for_money', shap_values, X_for_shap, interaction_index='cabin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if transform_dataset=='scaling_and_one_hot_encoding':\n",
    "    shap.dependence_plot('polarity', shap_values, X_for_shap, interaction_index='cabin_Economy Class')\n",
    "    shap.dependence_plot('polarity', shap_values, X_for_shap, interaction_index='cabin_Business Class')\n",
    "elif transform_dataset=='label_encoding':\n",
    "    shap.dependence_plot('polarity', shap_values, X_for_shap, interaction_index='cabin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if transform_dataset=='scaling_and_one_hot_encoding':\n",
    "    shap.dependence_plot('review_characters', shap_values, X_for_shap, interaction_index='cabin_Economy Class')\n",
    "    shap.dependence_plot('review_characters', shap_values, X_for_shap, interaction_index='cabin_Business Class')\n",
    "elif transform_dataset=='label_encoding':\n",
    "    shap.dependence_plot('review_characters', shap_values, X_for_shap, interaction_index='cabin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.8.4 - Decision plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SHAP decision plots** allow to visualize how a model arrives at a certain prediction, thus giving some insights on how decisions are made.  \n",
    "For each feature, from the bottom to the top, we see how the prediction changes when a certain feature is taken into account. The contribution of the features at the bottom is usually small (lower importance), while the contribution of the features at the top becomes larger and larger (higher importance).  \n",
    "Individual predictions can be highlighted using a dotted line style.   \n",
    "In the decision plot, we can look at the SHAP values or at the SHAP interaction values.  \n",
    "\n",
    "By looking at several predictions in an aggregated form, we can identify typical prediction paths.  \n",
    "For example, we can look for patterns among the most positive or most negative customer reviews, or look at the customer reviews that are misclassified by the model and try to understand why this is happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values[:20].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_for_shap.iloc[:20].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "shap.decision_plot(base_value=expected_value, shap_values=shap_values[:20], features=X_for_shap.iloc[:20], link='logit', color_bar=True, highlight=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if transform_dataset=='scaling_and_one_hot_encoding':\n",
    "    shap.decision_plot(base_value=expected_value, shap_values=shap_int_values[:20], features=X_for_shap.iloc[:20], link='logit', color_bar=True, highlight=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.decision_plot(base_value=expected_value, \n",
    "                   shap_values=shap_values_strong_rec, \n",
    "                   features=X_strong_rec, \n",
    "                   link='logit', \n",
    "                   color_bar=True, \n",
    "                   feature_order='hclust', \n",
    "                   ignore_warnings=True, \n",
    "                   xlim=(0.98,1.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.decision_plot(base_value=expected_value, \n",
    "                   shap_values=shap_values_strong_not_rec, \n",
    "                   features=X_strong_not_rec, \n",
    "                   link='logit', \n",
    "                   color_bar=True, \n",
    "                   feature_order='hclust', \n",
    "                   ignore_warnings=True,\n",
    "                   xlim=(0.,0.02))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.decision_plot(base_value=expected_value, shap_values=shap_values_misclassified, features=X_misclassified, link='logit', color_bar=True, feature_order='hclust', ignore_warnings=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.decision_plot(base_value=expected_value, \n",
    "                   shap_values=shap_values[df['traveller_type']=='Business'], \n",
    "                   features=X_for_shap[df['traveller_type']=='Business'], \n",
    "                   link='logit', \n",
    "                   color_bar=True, \n",
    "                   feature_order='hclust', \n",
    "                   ignore_warnings=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.8.5 - Force plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.force_plot(base_value=expected_value, shap_values=shap_values[0], features=X_for_shap.iloc[0], link='logit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_for_shap.iloc[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.force_plot(base_value=expected_value, shap_values=shap_values[20], features=X_for_shap.iloc[20], link='logit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_for_shap.iloc[20].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Save output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out.to_csv(df_out_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
